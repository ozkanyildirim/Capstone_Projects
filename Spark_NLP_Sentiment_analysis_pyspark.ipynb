{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit"
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "colab": {
      "name": "Spark_NLP_Sentiment_analysis_pyspark.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ozkanyildirim/Capstone_Projects/blob/master/Spark_NLP_Sentiment_analysis_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7Vld5BTZg6p"
      },
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPxbbRL6ZdHB"
      },
      "source": [
        "# Sentiment Analaysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv6ejDpmVvPk"
      },
      "source": [
        "Project Goal: Our goal is to build models with high accuracy to make correct predictions regarding given costemer comments, whether they are positive or negative.\n",
        "\n",
        "Data: **train data product reviews.csv** and **test data product reviews.csv** text data consisting of comments and bicathegorical labels.\n",
        "\n",
        "Table of contents\n",
        "\n",
        "1. Explarotory Data Analaysis (Preparing Train and Test Data)\n",
        "> I. Initials <br> \n",
        "> II. Data Preparation <br> \n",
        "> III. Data Visualisation <br> \n",
        "2. Sentiment Analysis\n",
        "> I. Logistic Regression and Naive Bayes with CountVectorizer <br> \n",
        "> II. Logistic Regression and Naive Bayes with TFIDF<br> \n",
        "> III. Universal Sentence Encoder <br> \n",
        "\n",
        "3. Conclusion\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIyEXymmaXuL"
      },
      "source": [
        "## 1. Explarotory Data Analaysis (Preparing Train and Test Data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tum--aDgbeec"
      },
      "source": [
        "> ### I.  Initials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JsBmi2osBAT"
      },
      "source": [
        ">> a. Importing Initial Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG_3zMIOY26q"
      },
      "source": [
        "# general purpose modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# pyspark modules\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from pyspark.ml.feature import CountVectorizer, HashingTF, IDF, OneHotEncoder, StringIndexer, VectorAssembler, SQLTransformer\n",
        "\n",
        "# spark nlp modules\n",
        "import sparknlp\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sCpNPlvd5oB"
      },
      "source": [
        ">> b.  Starting a Pyspark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6b2M980tsod"
      },
      "source": [
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54ocImEdsdzp"
      },
      "source": [
        "> ### II.  Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNvEeZwnekzL"
      },
      "source": [
        ">> a.  Retrieving Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt0eWvZCY26t"
      },
      "source": [
        "comments_train = spark.read.options(delimiter=';').csv('train data product reviews.csv', inferSchema=True, header=True)\n",
        "comments_train.show(truncate=True, n=5)\n",
        "comments_train.count(), comments_train.select('label').distinct().count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Zxwf2RY26u"
      },
      "source": [
        "In 'label' column we have 0's and 1's only. Let's rearrange this data frame as *df_train*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmJ8jkfMY26v"
      },
      "source": [
        "df_train = comments_train.select('text', 'label')\n",
        "df_train.show(truncate=True, n=5)\n",
        "df_train.groupBy('label').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP2qC4ibVZcC"
      },
      "source": [
        "df_train.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvTS15kIKCUH"
      },
      "source": [
        "def balance_check(df, col='label'):\n",
        "  \"\"\"\n",
        "  Checks the balance of data regarding labels and displays.\n",
        "  df: data frame\n",
        "  col: string column\n",
        "  \"\"\"\n",
        "  positive = df.where(df.label == '1').count()\n",
        "  negative = df.where(df.label == '0').count()\n",
        "  pos_percent = 100 * positive/(positive + negative)\n",
        "  neg_percent = 100 * negative/(positive + negative)\n",
        "  print(f'Positive Comments: {positive} which is %{pos_percent}')\n",
        "  print(f'Negative Comments: {negative} which is %{neg_percent}')\n",
        "\n",
        "balance_check(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plvshF7MNQNF"
      },
      "source": [
        "Given the distribution of the comments in training data we have a relative unbalanced data (~ 0.28 - 0.72). Before deciding whether applying a downsizing or upsizing technique, let's first check whether do we have duplications in the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFnyj8TbSJDc"
      },
      "source": [
        "import pyspark.sql.functions as funcs\n",
        "df_train.groupBy(df_train.text)\\\n",
        "    .count()\\\n",
        "    .where(funcs.col('count') > 1)\\\n",
        "    .select(funcs.sum('count'))\\\n",
        "    .show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE5Wbe1PWdjP"
      },
      "source": [
        "Let's drop the duplicated rows and keep only the first occurences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36o3jDvKW2G7"
      },
      "source": [
        "df_train = df_train.dropDuplicates((['text']))\n",
        "balance_check(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFlN0d8OXTYi"
      },
      "source": [
        "After removing the duplications, the distribution of comments in the training data changed slightly to the positive (more balanced ~ 0.33 - 0.67). For now, we keep the data in this distribution and do not apply any downsizing or upsizing technique (or generation), but we use the F1 score as a performance metric to avoid being biased by the data distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI3aj_VWY26w"
      },
      "source": [
        "Now we are going to maintain a *df_test* similar to *df_train*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df6JNFjJtH6T"
      },
      "source": [
        ">> b.  Retrieving Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJryBbfzY26w"
      },
      "source": [
        "comments_test = spark.read.options(delimiter=';').csv('test data product reviews.csv', inferSchema=True, header=True)\n",
        "comments_test.show(truncate=True, n=5)\n",
        "comments_test.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5YN-XmqY26x"
      },
      "source": [
        "We are going to use *regex* to describe patters to obtain a clean data frame with columns text and label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAQtHWMeY26y"
      },
      "source": [
        "regex_pattern = r'\"*([01])(.+)'\n",
        "comments_test = comments_test.withColumn('text', regexp_extract(col('label,text'), regex_pattern, 2))\\\n",
        "                 .withColumn('label', regexp_extract(col('label,text'), regex_pattern, 1))\n",
        "df_test = comments_test.select('text', 'label')\n",
        "df_test.show(truncate=True, n=5)\n",
        "df_test.count(), df_test.select('label').distinct().count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN_lvDlaYxfk"
      },
      "source": [
        "import pyspark.sql.functions as funcs\n",
        "df_test.groupBy(df_test.text)\\\n",
        "    .count()\\\n",
        "    .where(funcs.col('count') > 1)\\\n",
        "    .select(funcs.sum('count'))\\\n",
        "    .show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgIqCGX6ZC29"
      },
      "source": [
        "balance_check(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGx69lH8Yzln"
      },
      "source": [
        "Apperently we do not have duplications in test data. And our test data is balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQbwxIa6Y260"
      },
      "source": [
        "Now that we have both *df_train* and *df_test* in our targetted composition, we can progress with data visualisation and finally the **Sentiment Analysis**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBkUebbKtUn5"
      },
      "source": [
        "> ### III. Data Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQxujkfN07GY"
      },
      "source": [
        "import plotly.express as px"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aOSGjXg-COk"
      },
      "source": [
        "df_viz_train = df_train.toPandas()\n",
        "df_viz_train.to_csv('train_viz', sep='\\t', encoding='utf-8', index=False)\n",
        "df_viz_test = df_train.toPandas()\n",
        "df_viz_test.to_csv('test_viz', sep='\\t', encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcS1ff4CBIjd"
      },
      "source": [
        "# length of comment\n",
        "df_viz_train['length'] = df_viz_train.text.apply(lambda x: len(x))\n",
        "df_viz_train.head()\n",
        "df_viz_test['length'] = df_viz_test.text.apply(lambda x: len(x))\n",
        "df_viz_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CU-hRTcJdgQ"
      },
      "source": [
        "sns.displot(data=df_viz_train, x='label', y='length')\n",
        "plt.xticks([0,1], ['negative', 'positive'], rotation='vertical')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1v_YEGGOGOe"
      },
      "source": [
        "def clean_token_extractor(df):\n",
        "\n",
        "    '''\n",
        "    Returns three pandas Data Frames (for positive and negative comments seperately \n",
        "    and also all comments) with columns ['label', 'result'],\n",
        "    where 'result' contains cleaned tokens of words.\n",
        "    ''' \n",
        "    %%time\n",
        "    document_assembler = DocumentAssembler()\\\n",
        "          .setInputCol(\"text\")\\\n",
        "          .setOutputCol(\"document\")\n",
        "\n",
        "    tokenizer = Tokenizer()\\\n",
        "          .setInputCols([\"document\"])\\\n",
        "          .setOutputCol(\"token\")\n",
        "\n",
        "    normalizer = Normalizer()\\\n",
        "          .setInputCols([\"token\"])\\\n",
        "          .setOutputCol(\"normalized\")\n",
        "\n",
        "    stopwords_cleaner = StopWordsCleaner()\\\n",
        "          .setInputCols(\"normalized\")\\\n",
        "          .setOutputCol(\"cleanTokens\")\\\n",
        "          .setCaseSensitive(False)\n",
        "\n",
        "    pipe_viz = Pipeline(\n",
        "        stages=[document_assembler,\n",
        "                tokenizer,\n",
        "                normalizer,\n",
        "                stopwords_cleaner\n",
        "                ])\n",
        "\n",
        "    model_viz = pipe_viz.fit(df)\n",
        "    df_viz = model_viz.transform(df)\n",
        "\n",
        "    df_viz_all = df_viz.select('label','cleanTokens.result').toPandas()\n",
        "    df_viz_poz = df_viz_all[df_viz_all['label'] == 1]\n",
        "    df_viz_neg = df_viz_all[df_viz_all['label'] == 0]\n",
        "    return df_viz_poz, df_viz_neg, df_viz_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx8n8T1kUhFl"
      },
      "source": [
        "df_viz_train_pos, df_viz_train_neg, df_viz_train = clean_token_extractor(df_train)\n",
        "df_viz_test_pos, df_viz_test_neg, df_viz_test = clean_token_extractor(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mivjBXzYaM5e"
      },
      "source": [
        "def word_bag(df, col = 'result'):\n",
        "  \n",
        "    \"\"\"\n",
        "    Counts each word in a data frame and returns counts of each word in a \n",
        "    Pandas Data Frame\n",
        "    col: feature (each row list of strings)\n",
        "    \"\"\" \n",
        "    full_list = []  \n",
        "    for elmnt in df[col]:  \n",
        "        full_list += elmnt  \n",
        "\n",
        "    val_counts = pd.Series(full_list).value_counts()\n",
        "    df_words = pd.DataFrame(val_counts).reset_index().rename(columns={'index':'word', 0:'word_count'}).sort_values(by='word_count', ascending=False)\n",
        "    return df_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g523wO-pnw_"
      },
      "source": [
        "df_words_all = word_bag(df_viz_train)\n",
        "df_words_pos = word_bag(df_viz_train_pos)\n",
        "df_words_neg = word_bag(df_viz_train_neg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT9VdL_hpgrW"
      },
      "source": [
        "def word_displayer(df, n=20, stat='positive'):\n",
        "    '''\n",
        "    Visualise most frequent words in a Pandas Data Frame.\n",
        "    stat: 'positive', 'negative', 'all'\n",
        "    n: most frequent n words\n",
        "    '''\n",
        "    f, ax = plt.subplots(figsize=(10, 10))\n",
        "    sns.set_color_codes(\"pastel\")\n",
        "    sns.barplot(x=\"word_count\", y=\"word\", data=df.iloc[0:n,:],\n",
        "                label=\"Total\", color=\"b\")\n",
        "    ax.set(xlim=(100), ylabel=\"\",\n",
        "          xlabel=f\"Number of Words\")\n",
        "    ax.set_title(f'{stat.upper()} COMMENTS')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Xo5jrXrAfm"
      },
      "source": [
        "word_displayer(df=df_words_all, n=20, stat='all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbVBuminvr41"
      },
      "source": [
        "word_displayer(df=df_words_neg, n=20, stat='negative')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqoIIh5iybrv"
      },
      "source": [
        "word_displayer(df=df_words_pos, n=20, stat='positive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2ncjC1PZ9zw"
      },
      "source": [
        "## 2. Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpSJgnqxkt4t"
      },
      "source": [
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToEYKcCQmSz7"
      },
      "source": [
        "> ### I.  Logistic Regression and Naive Bayes with **CountVectorizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqTwqa-_jdb8"
      },
      "source": [
        ">> i. Building Pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnsGnbzfmeNm"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt\n",
        "from pyspark.ml.feature import CountVectorizer, HashingTF, IDF, OneHotEncoder, StringIndexer, VectorAssembler, SQLTransformer\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKHyFGc-m2Eb"
      },
      "source": [
        "%%time\n",
        "\n",
        "document_assembler = DocumentAssembler()\\\n",
        "      .setInputCol(\"text\")\\\n",
        "      .setOutputCol(\"document\")\n",
        "\n",
        "sentence = SentenceDetector()\\\n",
        "      .setInputCols(\"document\")\\\n",
        "      .setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer()\\\n",
        "      .setInputCols([\"sentence\"])\\\n",
        "      .setOutputCol(\"token\")\n",
        "\n",
        "normalizer = Normalizer()\\\n",
        "      .setInputCols([\"token\"])\\\n",
        "      .setOutputCol(\"normalized\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"normalized\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)\n",
        "\n",
        "\n",
        "stemmer = Stemmer()\\\n",
        "      .setInputCols([\"cleanTokens\"])\\\n",
        "      .setOutputCol(\"stem\")\n",
        "\n",
        "finisher = Finisher()\\\n",
        "      .setInputCols([\"stem\"])\\\n",
        "      .setOutputCols([\"token_features\"])\\\n",
        "      .setOutputAsArray(True)\\\n",
        "      .setCleanAnnotations(False)\n",
        "\n",
        "label_strIdx = StringIndexer(inputCol='label', outputCol='target')\n",
        "logReg = LogisticRegression(maxIter=5, regParam=0.01)\n",
        "naiveBayes = NaiveBayes(smoothing=5)\n",
        "countVectors = CountVectorizer(inputCol=\"token_features\", outputCol=\"features\", vocabSize=10000, minDF=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rX-T4nVlG4W"
      },
      "source": [
        ">> ii. Forming Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0nXg0QYu3aC"
      },
      "source": [
        "# Pipeline for Logistic Regression with CountVectorizer\n",
        "nlp_pipeline_cv_lr = Pipeline(\n",
        "    stages=[document_assembler,\n",
        "            sentence,\n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner, \n",
        "            stemmer, \n",
        "            finisher,\n",
        "            countVectors,\n",
        "            logReg\n",
        "            ])\n",
        "\n",
        "# Pipeline for Naive Bayes with CountVectorizer\n",
        "nlp_pipeline_cv_nb = Pipeline(\n",
        "    stages=[document_assembler,\n",
        "            sentence,\n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner, \n",
        "            stemmer, \n",
        "            finisher,\n",
        "            countVectors,\n",
        "            naiveBayes\n",
        "            ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kh_CEDLfvzh"
      },
      "source": [
        "\n",
        ">> iii. Logistic Regression with CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o17HwXsece77"
      },
      "source": [
        ">>> a. Applying LogReg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSPb0mYKhpZm"
      },
      "source": [
        "modelLR = nlp_pipeline_cv_lr.fit(df_train)\n",
        "pred_lr = modelLR.transform(df_test)\n",
        "pred_lr = pred_lr.withColumn('label', pred_lr.label.cast(IntegerType()))\n",
        "pred_lr.filter(pred_lr['prediction'] == 0)\\\n",
        "    .select(\"text\",\"probability\",\"label\",\"prediction\")\\\n",
        "    .orderBy(\"probability\", ascending=False)\\\n",
        "    .show(n = 10, truncate = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kDyh4huoBeN"
      },
      "source": [
        ">>> b. Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TyxypTfn51g"
      },
      "source": [
        "# Converting pred_lr to pandas data frame in order to using sklearn metrics library\n",
        "df_lr = pred_lr.select('text','label','prediction').toPandas()\n",
        "print(classification_report(df_lr.label, df_lr.prediction))\n",
        "print(accuracy_score(df_lr.label, df_lr.prediction))\n",
        "\n",
        "# Evaluation within the Spark Universe is also possible (for scaling issues)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol=\"prediction\")\n",
        "evaluator.evaluate(pred_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS3O0kMGhgeo"
      },
      "source": [
        "\n",
        ">> iv. Naive Bayes with CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8RobQnAhgew"
      },
      "source": [
        ">>> a. Applying Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4txcr2rhgew"
      },
      "source": [
        "modelNB = nlp_pipeline_cv_nb.fit(df_train)\n",
        "pred_nb = modelNB.transform(df_test)\n",
        "pred_nb = pred_nb.withColumn('label', pred_nb.label.cast(IntegerType()))\n",
        "pred_nb.filter(pred_nb['prediction'] == 0)\\\n",
        "    .select(\"text\",\"probability\",\"label\",\"prediction\")\\\n",
        "    .orderBy(\"probability\", ascending=False)\\\n",
        "    .show(n = 10, truncate = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TD2E-8Ihgex"
      },
      "source": [
        ">>> b. Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj3TcdCfhgey"
      },
      "source": [
        "# Converting pred_nb to pandas data frame in order to using sklearn metrics library\n",
        "df_nb = pred_nb.select('text','label','prediction').toPandas()\n",
        "print(classification_report(df_nb.label, df_nb.prediction))\n",
        "print(accuracy_score(df_nb.label, df_nb.prediction))\n",
        "\n",
        "# Evaluation within the Spark Universe is also possible (for scaling issues)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol=\"prediction\")\n",
        "evaluator.evaluate(pred_nb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8Mxak03k6V8"
      },
      "source": [
        "> ### II.  **TFIDF** Logistic Regression and Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3Q2y4Trk6WF"
      },
      "source": [
        ">> i. Building Pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPoz2Qa8k6WF"
      },
      "source": [
        "from pyspark.ml.feature import CountVectorizer, HashingTF, IDF, OneHotEncoder, StringIndexer, VectorAssembler, SQLTransformer\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeW-EpNrk6WG"
      },
      "source": [
        "%%time\n",
        "document_assembler = DocumentAssembler()\\\n",
        "      .setInputCol(\"text\")\\\n",
        "      .setOutputCol(\"document\")\n",
        "\n",
        "sentence = SentenceDetector()\\\n",
        "      .setInputCols(\"document\")\\\n",
        "      .setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer()\\\n",
        "      .setInputCols([\"sentence\"])\\\n",
        "      .setOutputCol(\"token\")\n",
        "\n",
        "normalizer = Normalizer()\\\n",
        "      .setInputCols([\"token\"])\\\n",
        "      .setOutputCol(\"normalized\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"normalized\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)\n",
        "\n",
        "\n",
        "stemmer = Stemmer()\\\n",
        "      .setInputCols([\"cleanTokens\"])\\\n",
        "      .setOutputCol(\"stem\")\n",
        "\n",
        "finisher = Finisher()\\\n",
        "      .setInputCols([\"stem\"])\\\n",
        "      .setOutputCols([\"token_features\"])\\\n",
        "      .setOutputAsArray(True)\\\n",
        "      .setCleanAnnotations(False)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"token_features\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n",
        "label_strIdx = StringIndexer(inputCol='label', outputCol='target')\n",
        "logReg = LogisticRegression(maxIter=5, regParam=0.01)\n",
        "naiveBayes = NaiveBayes(smoothing=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CclnoSPok6WG"
      },
      "source": [
        ">> ii. Forming Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iPeU0MVk6WG"
      },
      "source": [
        "# Pipeline for Logistic Regression with TFIDF\n",
        "nlp_pipeline_tf_lr = Pipeline(\n",
        "    stages=[document_assembler,\n",
        "            sentence,\n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner, \n",
        "            stemmer, \n",
        "            finisher,\n",
        "            hashingTF,\n",
        "            idf,\n",
        "            logReg\n",
        "            ])\n",
        "\n",
        "# Pipeline for Naive Bayes with TFIDF\n",
        "nlp_pipeline_tf_nb = Pipeline(\n",
        "    stages=[document_assembler,\n",
        "            sentence,\n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner, \n",
        "            stemmer, \n",
        "            finisher,\n",
        "            hashingTF,\n",
        "            idf,\n",
        "            naiveBayes\n",
        "            ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhvxqkyZk6WH"
      },
      "source": [
        "\n",
        ">> iii. Logistic Regression with TFIDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvjdXiMYk6WI"
      },
      "source": [
        ">>> a. Applying LogReg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a55rXBNBk6WJ"
      },
      "source": [
        "modelLR = nlp_pipeline_tf_lr.fit(df_train)\n",
        "pred_tf_lr = modelLR.transform(df_test)\n",
        "pred_tf_lr = pred_tf_lr.withColumn('label', pred_tf_lr.label.cast(IntegerType()))\n",
        "pred_tf_lr.filter(pred_tf_lr['prediction'] == 0)\\\n",
        "    .select(\"text\",\"probability\",\"label\",\"prediction\")\\\n",
        "    .orderBy(\"probability\", ascending=False)\\\n",
        "    .show(n = 10, truncate = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5KgC2XOk6WJ"
      },
      "source": [
        ">>> b. Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bE8mBwFk6WK"
      },
      "source": [
        "# Converting pred_tf_lr to pandas data frame in order to using sklearn metrics library\n",
        "df_tf_lr = pred_tf_lr.select('text','label','prediction').toPandas()\n",
        "print(classification_report(df_tf_lr.label, df_tf_lr.prediction))\n",
        "print(accuracy_score(df_tf_lr.label, df_tf_lr.prediction))\n",
        "\n",
        "# Evaluation within the Spark Universe is also possible (for scaling issues)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol=\"prediction\")\n",
        "evaluator.evaluate(pred_tf_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EQI-djwk6WL"
      },
      "source": [
        "\n",
        ">> iv. Naive Bayes with TFIDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MlycYByk6WL"
      },
      "source": [
        ">>> a. Applying Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj4Ixqv3k6WL"
      },
      "source": [
        "modelNB = nlp_pipeline_tf_nb.fit(df_train)\n",
        "pred_tf_nb = modelNB.transform(df_test)\n",
        "pred_tf_nb = pred_tf_nb.withColumn('label', pred_tf_nb.label.cast(IntegerType()))\n",
        "pred_tf_nb.filter(pred_tf_nb['prediction'] == 0)\\\n",
        "    .select(\"text\",\"probability\",\"label\",\"prediction\")\\\n",
        "    .orderBy(\"probability\", ascending=False)\\\n",
        "    .show(n = 10, truncate = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDcGMIhAk6WM"
      },
      "source": [
        ">>> b. Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IztXIR9ok6WM"
      },
      "source": [
        "# Converting pred_tf_nb to pandas data frame in order to using sklearn metrics library\n",
        "df_tf_nb = pred_tf_nb.select('text','label','prediction').toPandas()\n",
        "print(classification_report(df_lr.label, df_lr.prediction))\n",
        "print(accuracy_score(df_lr.label, df_lr.prediction))\n",
        "\n",
        "# Evaluation within the Spark Universe is also possible (for scaling issues)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol=\"prediction\")\n",
        "evaluator.evaluate(pred_tf_nb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEjzQFbU8Wu2"
      },
      "source": [
        "> ### III.  Universal Sentence Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4qvCNvv8Wu3"
      },
      "source": [
        ">> i. Building Pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXlrCAWL8Wu3"
      },
      "source": [
        "%%time\n",
        "\n",
        "document = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "    \n",
        "use = UniversalSentenceEncoder.pretrained()\\\n",
        " .setInputCols([\"document\"])\\\n",
        " .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "classsifierdl = ClassifierDLApproach()\\\n",
        "  .setInputCols([\"sentence_embeddings\"])\\\n",
        "  .setOutputCol(\"class\")\\\n",
        "  .setLabelColumn(\"label\")\\\n",
        "  .setMaxEpochs(11)\\\n",
        "  .setEnableOutputLogs(True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCGdeeTQ8Wu3"
      },
      "source": [
        ">> ii. Forming Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmCN2GEd8Wu4"
      },
      "source": [
        "use_clf_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        use,\n",
        "        classsifierdl\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7RvbIl7FRDB"
      },
      "source": [
        "!cd ~/annotator_logs && ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rie1VFU08Wu4"
      },
      "source": [
        "\n",
        ">> iii. Universal Sentence Encoder with Deep Learning Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vKojr8D8Wu4"
      },
      "source": [
        ">>> a. Applying Universal Sentence Encoder with DL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JuGugAm8Wu4"
      },
      "source": [
        "useModel = use_clf_pipeline.fit(df_train)\n",
        "pred_use = useModel.transform(df_test)\n",
        "df_use = pred_use.select('text','label', 'class.result').toPandas()\n",
        "df_use['result'] = df_use['result'].apply(lambda x: x[0])\n",
        "df_use.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMa0rrCm8Wu4"
      },
      "source": [
        ">>> b. Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ2vDCDg8Wu5"
      },
      "source": [
        "print(classification_report(df_use.label, df_use.result))\n",
        "print(accuracy_score(df_use.label, df_use.result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDA-Htxfmz0g"
      },
      "source": [
        "# 3. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pgkhL0h1tlc"
      },
      "source": [
        "Considering the wide application areas of NLP, building solid algorithms on a large scale is a powerful asset that strengthens the competence of companies in almost all business areas. From this perspective, I approached this simulation of ESPRIT's use case, keeping in touch with 3 pillars.\n",
        "\n",
        "The first pillar could be described as \"keeping up with the pace of NLP inferno\". In other words, I applied modern embedding techniques like \"Universal Sendence Embedding\" which use pre-trained embedding algorithms powered by Deep Learning under the hood. Not to my surprise, I got the best accuracy results using \"Universal Sendence Embedding\". But also I needed to use resources effectively and did not used alternatives like Bert Centence Embedding (Note that for code readability reasons, no fine tuning steps are included in this notebook). \n",
        "\n",
        "The second pillar is \"simplicity\" and this is where pipelines comes into play. I built pipelines that made my code modular, digestible and also stable. My team partners could easily apply/improve my code without any additional support, which cannot be overstated.\n",
        "\n",
        "The third pillar is 'extending the horizon' which could be explained observing the wider possibilities and sharing these new horizons with my colleagues. To do so I attended the John Snow Labs NLP for Data Science workshop (live-online) and tried to apply these new visions in my case study.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq9y-ixB-zay"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}